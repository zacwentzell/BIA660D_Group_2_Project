{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import scipy.sparse as sparse\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "re_spaces = re.compile(r'\\s+')\n",
    "\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned_reviews(x):\n",
    "    return(''.join(re.sub('[^a-zA-Z_]', ' ', x)))\n",
    "    #return(''.join(re.sub('[^a-zA-Z_]', ' ', x).replace(':',' ').lower().replace('\\r',' ').replace('!',' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "def get_bigram_likelihood(statements, freq_filter=3, nbest=200):\n",
    "    \"\"\"\n",
    "    Returns n (likelihood ratio) bi-grams from a group of documents\n",
    "    :param        statements: list of strings\n",
    "    :param output_file: output path for saved file\n",
    "    :param freq_filter: filter for # of appearances in bi-gram\n",
    "    :param       nbest: likelihood ratio for bi-grams\n",
    "    \"\"\"\n",
    "\n",
    "    #words = list()\n",
    "    #tokenize sentence into words\n",
    "    #for statement in statements:\n",
    "        # remove non-words\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(statements)\n",
    "\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "\n",
    "    # only bi-grams that appear n+ times\n",
    "    bigram_finder.apply_freq_filter(freq_filter)\n",
    "\n",
    "    # TODO: use custom stop words\n",
    "    bigram_finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "    bigram_results = bigram_finder.nbest(bigram_measures.likelihood_ratio, nbest)\n",
    "\n",
    "    return bigram_finder.score_ngrams(bigram_measures.likelihood_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_data = pd.read_csv('All_Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = table_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.rename(index=str, columns={\"Unnamed: 0\": \"ID\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Ratings'] = df2['Ratings'].apply(lambda x: str(x).split()[0]).apply(lambda x: pd.to_numeric(x, errors='coerce'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Ratings'].sort_values().reset_index(drop=True).dropna().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Reviews'] = df2['Reviews'].apply(lambda x : cleaned_reviews(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2['Reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Comments'] = df2['Reviews'].apply(lambda x: np.concatenate(np.array([word_tokenize(x)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2['Comments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reattach_contractions(wordlist):\n",
    "    words = []\n",
    "    for i, word in enumerate(wordlist):\n",
    "        if word[0] == \"'\" or word == \"n't\":\n",
    "            words[-1] = words[-1] + word\n",
    "        else:\n",
    "            words.append(word)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2['Comments'] = df2['Comments'].apply(lambda x: reattach_contractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2['Comments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigramify(words):\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(words)\n",
    "    finder.apply_freq_filter(3) \n",
    "    return finder.nbest(bigram_measures.pmi, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Comments'] = df2['Comments'].apply(lambda x: bigramify(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2['Comments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_reviews(id):\n",
    "    bigrams_array = df2[df2['ID'] == id]['Comments'].values\n",
    "    review_texts = df2[df2['ID'] == id]['Reviews'].values\n",
    "    bigrams_list = bigrams_array.tolist()\n",
    "    bigrams = []\n",
    "    for item in bigrams_list:\n",
    "        for x in item:\n",
    "            bigrams.append(x)\n",
    "    if bigrams:\n",
    "        sample_reviews = []\n",
    "        review_texts = review_texts\n",
    "        for bigram in bigrams:\n",
    "            sample_review_list = list(filter(lambda txt: \" \".join(bigram) in txt, review_texts))\n",
    "            num_reviews = len(sample_review_list)\n",
    "            if num_reviews != 0:\n",
    "                sample_review = sample_review_list[0]\n",
    "                sample_review = sample_review.replace(\" \".join(bigram), \"****\" + \" \".join(bigram) + \"****\")\n",
    "                start_index = sample_review.index(\"****\")\n",
    "                sample_text = sample_review[start_index - len(sample_review): start_index + len(sample_review)]\n",
    "                sample_reviews.append(sample_text)\n",
    "        return sample_reviews\n",
    "    else:\n",
    "        return (list(review_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2['Comments'] = df2['ID'].apply(lambda x: sample_reviews(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2['Comments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2['Comments'] = df2['Comments'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2['Comments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_sentiment(string):\n",
    "    sent = analyser.polarity_scores(string)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preCol = df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posCol = df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preCol['Sentiments'] = df_preCol['Reviews'].apply(lambda x: review_sentiment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posCol['Sentiments'] = df_posCol['Comments'].apply(lambda x: review_sentiment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preCol = pd.concat([df_preCol.drop(['Sentiments'], axis=1), df_preCol['Sentiments'].apply(pd.Series)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_posCol = pd.concat([df_posCol.drop(['Sentiments'], axis=1), df_posCol['Sentiments'].apply(pd.Series)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_preCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_posCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_reviews = pd.DataFrame()\n",
    "scored_reviews['review'] = df_posCol['Comments']\n",
    "scored_reviews['compound'] = df_posCol['compound']\n",
    "scored_reviews['negativity'] = df_posCol['neg']\n",
    "scored_reviews['neutrality'] = df_posCol['neu']\n",
    "scored_reviews['positivity'] = df_posCol['pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(scored_reviews['neutrality']).plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(scored_reviews['positivity']).plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(scored_reviews['negativity']).plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scored_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scored_reviews.query('negativity > 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scored_reviews.query('negativity > positivity').query('negativity > 0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scored_reviews.query('negativity > positivity').query('compound < -0.2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
